{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMwCm867z6RSaoQ7bUCpdb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiMasram750/Tree_Species_Classification/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ConHTt1Hu_Z_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"viditgandhi/tree-species-identification-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "Zk0gR9l5vFoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55119e34-ee6f-47e5-e7d1-c0f6660403e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/tree-species-identification-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ce77fa1",
        "outputId": "37203a04-a216-48b4-c4af-d2fd4d29b08f"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the base directory for the dataset\n",
        "dataset_path = '/kaggle/input/tree-species-identification-dataset'\n",
        "image_dir = os.path.join(dataset_path) # Corrected path\n",
        "\n",
        "# Create a list to store image paths and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through the subdirectories (each represents a tree species)\n",
        "for label in os.listdir(image_dir):\n",
        "    label_dir = os.path.join(image_dir, label)\n",
        "    if os.path.isdir(label_dir):\n",
        "        for image_file in os.listdir(label_dir):\n",
        "            image_path = os.path.join(label_dir, image_file)\n",
        "            image_paths.append(image_path)\n",
        "            labels.append(label)\n",
        "\n",
        "# Create a pandas DataFrame\n",
        "df_images = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
        "\n",
        "# Create a simple label mapping\n",
        "unique_labels = sorted(df_images['label'].unique())\n",
        "label_map = {label: i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "print(\"Data cleaning and preprocessing (simple approach) complete.\")\n",
        "print(f\"Number of images found: {len(df_images)}\")\n",
        "print(f\"Number of unique species: {len(unique_labels)}\")\n",
        "print(\"Label mapping:\", label_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data cleaning and preprocessing (simple approach) complete.\n",
            "Number of images found: 31\n",
            "Number of unique species: 1\n",
            "Label mapping: {'Tree_Species_Dataset': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "789GEos00IOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "07b3e220",
        "outputId": "73a93ce7-3e6f-4ebe-82d1-74393ed1caf7"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation (optional but recommended)\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2  # Split 20% of data for validation\n",
        ")\n",
        "\n",
        "# Create data generators for training and validation\n",
        "train_generator = datagen.flow_from_dataframe(\n",
        "    df_images,\n",
        "    x_col='image_path',\n",
        "    y_col='label',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',  # Use 'categorical' for one-hot encoded labels\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_dataframe(\n",
        "    df_images,\n",
        "    x_col='image_path',\n",
        "    y_col='label',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Define the model (simple CNN)\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(unique_labels), activation='softmax')  # Output layer with number of unique labels\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',  # Use 'categorical_crossentropy' for one-hot encoded labels\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 10  # You can adjust the number of epochs\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_images' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2793277176.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Create data generators for training and validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m train_generator = datagen.flow_from_dataframe(\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdf_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mx_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'image_path'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0my_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "bee9d292",
        "outputId": "ef1cd55d-004b-4840-cdbe-27df404b47b2"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the base directory for the dataset\n",
        "dataset_path = '/kaggle/input/tree-species-identification-dataset'\n",
        "image_dir = os.path.join(dataset_path)\n",
        "\n",
        "# Create a list to store image paths and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through the subdirectories (each represents a tree species)\n",
        "for label in os.listdir(image_dir):\n",
        "    label_dir = os.path.join(image_dir, label)\n",
        "    if os.path.isdir(label_dir):\n",
        "        for image_file in os.listdir(label_dir):\n",
        "            image_path = os.path.join(label_dir, image_file)\n",
        "            image_paths.append(image_path)\n",
        "            labels.append(label)\n",
        "\n",
        "# Create a pandas DataFrame\n",
        "df_images = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
        "\n",
        "# Create a simple label mapping\n",
        "unique_labels = sorted(df_images['label'].unique())\n",
        "label_map = {label: i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "print(\"Data cleaning and preprocessing (simple approach) complete.\")\n",
        "print(f\"Number of images found: {len(df_images)}\")\n",
        "print(f\"Number of unique species: {len(unique_labels)}\")\n",
        "print(\"Label mapping:\", label_map)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/tree-species-identification-dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-564459048.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Iterate through the subdirectories (each represents a tree species)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mlabel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/tree-species-identification-dataset'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "a99875d0",
        "outputId": "0a3ad6e2-c12c-46ce-dd30-481eba45424c"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation (optional but recommended)\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2  # Split 20% of data for validation\n",
        ")\n",
        "\n",
        "# Create data generators for training and validation\n",
        "train_generator = datagen.flow_from_dataframe(\n",
        "    df_images,\n",
        "    x_col='image_path',\n",
        "    y_col='label',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',  # Use 'categorical' for one-hot encoded labels\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_dataframe(\n",
        "    df_images,\n",
        "    x_col='image_path',\n",
        "    y_col='label',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Define the model (simple CNN)\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(unique_labels), activation='softmax')  # Output layer with number of unique labels\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',  # Use 'categorical_crossentropy' for one-hot encoded labels\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 10  # You can adjust the number of epochs\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_images' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2793277176.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Create data generators for training and validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m train_generator = datagen.flow_from_dataframe(\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdf_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mx_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'image_path'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0my_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "b2eb1aaa",
        "outputId": "4f0fa52a-d15c-46e2-8b46-a51e46410116"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the base directory for the dataset using the path from the download step\n",
        "# dataset_path = '/kaggle/input/tree-species-identification-dataset' # Original line\n",
        "dataset_path = path # Use the path variable from kagglehub.dataset_download\n",
        "image_dir = os.path.join(dataset_path)\n",
        "\n",
        "# Create a list to store image paths and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through the subdirectories (each represents a tree species)\n",
        "# Check if the directory exists before listing its contents\n",
        "if os.path.isdir(image_dir):\n",
        "    for label in os.listdir(image_dir):\n",
        "        label_dir = os.path.join(image_dir, label)\n",
        "        if os.path.isdir(label_dir):\n",
        "            for image_file in os.listdir(label_dir):\n",
        "                image_path = os.path.join(label_dir, image_file)\n",
        "                # Check if the file is an image (you might need a more robust check)\n",
        "                if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    image_paths.append(image_path)\n",
        "                    labels.append(label)\n",
        "else:\n",
        "    print(f\"Error: Dataset directory not found at {image_dir}\")\n",
        "\n",
        "\n",
        "# Create a pandas DataFrame\n",
        "df_images = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
        "\n",
        "# Create a simple label mapping\n",
        "unique_labels = sorted(df_images['label'].unique())\n",
        "label_map = {label: i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "print(\"Data cleaning and preprocessing (simple approach) complete.\")\n",
        "print(f\"Number of images found: {len(df_images)}\")\n",
        "print(f\"Number of unique species: {len(unique_labels)}\")\n",
        "print(\"Label mapping:\", label_map)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2021183884.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define the base directory for the dataset using the path from the download step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# dataset_path = '/kaggle/input/tree-species-identification-dataset' # Original line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;31m# Use the path variable from kagglehub.dataset_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mimage_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07fd717c",
        "outputId": "46b88ea9-7e83-4189-f9af-d8580216dd10"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation (optional but recommended)\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2  # Split 20% of data for validation\n",
        ")\n",
        "\n",
        "# Create data generators for training and validation\n",
        "# Ensure df_images is not empty before creating generators\n",
        "if not df_images.empty:\n",
        "    train_generator = datagen.flow_from_dataframe(\n",
        "        df_images,\n",
        "        x_col='image_path',\n",
        "        y_col='label',\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',  # Use 'categorical' for one-hot encoded labels\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "    validation_generator = datagen.flow_from_dataframe(\n",
        "        df_images,\n",
        "        x_col='image_path',\n",
        "        y_col='label',\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        subset='validation'\n",
        "    )\n",
        "\n",
        "    # Define the model (simple CNN)\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(len(unique_labels), activation='softmax')  # Output layer with number of unique labels\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',  # Use 'categorical_crossentropy' for one-hot encoded labels\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    epochs = 10  # You can adjust the number of epochs\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=epochs,\n",
        "        validation_data=validation_generator\n",
        "    )\n",
        "else:\n",
        "    print(\"No images found in the dataset directory. Cannot train the model.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No images found in the dataset directory. Cannot train the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cba0f15d",
        "outputId": "3c3df177-e3a4-4b52-b96d-e85886f06ca5"
      },
      "source": [
        "import os\n",
        "\n",
        "dataset_path = '/kaggle/input/tree-species-identification-dataset'\n",
        "\n",
        "# List the contents of the main dataset directory\n",
        "print(f\"Contents of {dataset_path}:\")\n",
        "if os.path.isdir(dataset_path):\n",
        "    print(os.listdir(dataset_path))\n",
        "    # If there are subdirectories, list the contents of the first few to get an idea of the structure\n",
        "    for item in os.listdir(dataset_path)[:5]: # List contents of first 5 items\n",
        "        item_path = os.path.join(dataset_path, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"\\nContents of subdirectory {item}:\")\n",
        "            print(os.listdir(item_path))\n",
        "else:\n",
        "    print(\"Dataset directory not found.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of /kaggle/input/tree-species-identification-dataset:\n",
            "Dataset directory not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bffc7ce5",
        "outputId": "7ca49e78-38e4-4e4a-a6f7-c1aa06f0a0f4"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "\n",
        "# Define the base directory for the dataset by downloading it again\n",
        "# This ensures the 'path' variable is available in this cell\n",
        "path = kagglehub.dataset_download(\"viditgandhi/tree-species-identification-dataset\")\n",
        "dataset_path = path\n",
        "# Update image_dir to point to the subdirectory containing the species folders\n",
        "image_dir = os.path.join(dataset_path, 'Tree_Species_Dataset')\n",
        "\n",
        "\n",
        "# Create a list to store image paths and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through the subdirectories (each represents a tree species)\n",
        "# Check if the directory exists before listing its contents\n",
        "if os.path.isdir(image_dir):\n",
        "    for label in os.listdir(image_dir):\n",
        "        label_dir = os.path.join(image_dir, label)\n",
        "        if os.path.isdir(label_dir):\n",
        "            for image_file in os.listdir(label_dir):\n",
        "                image_path = os.path.join(label_dir, image_file)\n",
        "                # Check if the file is an image (you might need a more robust check)\n",
        "                if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    image_paths.append(image_path)\n",
        "                    labels.append(label)\n",
        "else:\n",
        "    print(f\"Error: Dataset directory not found at {image_dir}\")\n",
        "\n",
        "\n",
        "# Create a pandas DataFrame\n",
        "df_images = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
        "\n",
        "# Create a simple label mapping\n",
        "unique_labels = sorted(df_images['label'].unique())\n",
        "label_map = {label: i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "print(\"Data cleaning and preprocessing (simple approach) complete.\")\n",
        "print(f\"Number of images found: {len(df_images)}\")\n",
        "print(f\"Number of unique species: {len(unique_labels)}\")\n",
        "print(\"Label mapping:\", label_map)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data cleaning and preprocessing (simple approach) complete.\n",
            "Number of images found: 1600\n",
            "Number of unique species: 30\n",
            "Label mapping: {'amla': 0, 'asopalav': 1, 'babul': 2, 'bamboo': 3, 'banyan': 4, 'bili': 5, 'cactus': 6, 'champa': 7, 'coconut': 8, 'garmalo': 9, 'gulmohor': 10, 'gunda': 11, 'jamun': 12, 'kanchan': 13, 'kesudo': 14, 'khajur': 15, 'mango': 16, 'motichanoti': 17, 'neem': 18, 'nilgiri': 19, 'other': 20, 'pilikaren': 21, 'pipal': 22, 'saptaparni': 23, 'shirish': 24, 'simlo': 25, 'sitafal': 26, 'sonmahor': 27, 'sugarcane': 28, 'vad': 29}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11b83686",
        "outputId": "bdc75579-064b-4c82-d22c-04ff289c4a58"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation (optional but recommended)\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2  # Split 20% of data for validation\n",
        ")\n",
        "\n",
        "# Create data generators for training and validation\n",
        "# Ensure df_images is not empty before creating generators\n",
        "if not df_images.empty:\n",
        "    train_generator = datagen.flow_from_dataframe(\n",
        "        df_images,\n",
        "        x_col='image_path',\n",
        "        y_col='label',\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',  # Use 'categorical' for one-hot encoded labels\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "    validation_generator = datagen.flow_from_dataframe(\n",
        "        df_images,\n",
        "        x_col='image_path',\n",
        "        y_col='label',\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        subset='validation'\n",
        "    )\n",
        "\n",
        "    # Define the model (simple CNN)\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(len(unique_labels), activation='softmax')  # Output layer with number of unique labels\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',  # Use 'categorical_crossentropy' for one-hot encoded labels\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    epochs = 10  # You can adjust the number of epochs\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=epochs,\n",
        "        validation_data=validation_generator\n",
        "    )\n",
        "else:\n",
        "    print(\"No images found in the dataset directory. Cannot train the model.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No images found in the dataset directory. Cannot train the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceb894d9",
        "outputId": "d37c0090-689b-42ab-da0d-31e3f52febe3"
      },
      "source": [
        "import os\n",
        "import kagglehub\n",
        "\n",
        "# Download the dataset again to get the latest path\n",
        "path = kagglehub.dataset_download(\"viditgandhi/tree-species-identification-dataset\")\n",
        "dataset_path = path\n",
        "\n",
        "# List the contents of the main dataset directory\n",
        "print(f\"Contents of {dataset_path}:\")\n",
        "if os.path.isdir(dataset_path):\n",
        "    print(os.listdir(dataset_path))\n",
        "    # If there are subdirectories, list the contents of the first few to get an idea of the structure\n",
        "    for item in os.listdir(dataset_path)[:5]: # List contents of first 5 items\n",
        "        item_path = os.path.join(dataset_path, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"\\nContents of subdirectory {item}:\")\n",
        "            print(os.listdir(item_path))\n",
        "else:\n",
        "    print(\"Dataset directory not found.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of /kaggle/input/tree-species-identification-dataset:\n",
            "['Tree_Species_Dataset']\n",
            "\n",
            "Contents of subdirectory Tree_Species_Dataset:\n",
            "['vad', 'saptaparni', 'kanchan', 'champa', 'cactus', 'other', 'pilikaren', 'garmalo', 'gunda', 'simlo', 'gulmohor', 'coconut', 'bili', 'neem', 'nilgiri', 'banyan', 'bamboo', 'motichanoti', 'amla', 'jamun', 'babul', 'sonmahor', 'shirish', 'sitafal', '.git', 'sugarcane', 'pipal', 'khajur', 'kesudo', 'mango', 'asopalav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba2ad73b",
        "outputId": "49f6b4c2-9357-4569-b0d7-b4fd09afc6ff"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation (optional but recommended)\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2  # Split 20% of data for validation\n",
        ")\n",
        "\n",
        "# Create data generators for training and validation\n",
        "# Ensure df_images is not empty before creating generators\n",
        "if not df_images.empty:\n",
        "    train_generator = datagen.flow_from_dataframe(\n",
        "        df_images,\n",
        "        x_col='image_path',\n",
        "        y_col='label',\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',  # Use 'categorical' for one-hot encoded labels\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "    validation_generator = datagen.flow_from_dataframe(\n",
        "        df_images,\n",
        "        x_col='image_path',\n",
        "        y_col='label',\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        subset='validation'\n",
        "    )\n",
        "\n",
        "    # Define the model (simple CNN)\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(len(unique_labels), activation='softmax')  # Output layer with number of unique labels\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',  # Use 'categorical_crossentropy' for one-hot encoded labels\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    epochs = 10  # You can adjust the number of epochs\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=epochs,\n",
        "        validation_data=validation_generator\n",
        "    )\n",
        "else:\n",
        "    print(\"No images found in the dataset directory. Cannot train the model.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1280 validated image filenames belonging to 30 classes.\n",
            "Found 320 validated image filenames belonging to 30 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 713ms/step - accuracy: 0.0378 - loss: 3.4171 - val_accuracy: 0.0281 - val_loss: 4.8765\n",
            "Epoch 2/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 677ms/step - accuracy: 0.1054 - loss: 3.0789 - val_accuracy: 0.1094 - val_loss: 9.2196\n",
            "Epoch 3/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 672ms/step - accuracy: 0.2050 - loss: 2.7002 - val_accuracy: 0.1531 - val_loss: 10.4629\n",
            "Epoch 4/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 667ms/step - accuracy: 0.3241 - loss: 2.3317 - val_accuracy: 0.1406 - val_loss: 11.3484\n",
            "Epoch 5/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 679ms/step - accuracy: 0.4458 - loss: 1.9206 - val_accuracy: 0.1844 - val_loss: 12.5736\n",
            "Epoch 6/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 676ms/step - accuracy: 0.5537 - loss: 1.5845 - val_accuracy: 0.1656 - val_loss: 15.8532\n",
            "Epoch 7/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 673ms/step - accuracy: 0.6768 - loss: 1.1505 - val_accuracy: 0.1500 - val_loss: 17.9642\n",
            "Epoch 8/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 693ms/step - accuracy: 0.7676 - loss: 0.8320 - val_accuracy: 0.1531 - val_loss: 21.8914\n",
            "Epoch 9/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 709ms/step - accuracy: 0.8789 - loss: 0.4860 - val_accuracy: 0.1562 - val_loss: 21.8787\n",
            "Epoch 10/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 782ms/step - accuracy: 0.8851 - loss: 0.3972 - val_accuracy: 0.1406 - val_loss: 28.2275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b628d31",
        "outputId": "d6ff691f-5d48-47a0-894c-13cce002543a"
      },
      "source": [
        "# Save the trained model\n",
        "model.save('tree_species_model.h5')\n",
        "\n",
        "print(\"Model saved successfully as tree_species_model.h5\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully as tree_species_model.h5\n"
          ]
        }
      ]
    }
  ]
}